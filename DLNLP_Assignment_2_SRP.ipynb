{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirtheshjain/cs772-Assignments/blob/main/DLNLP_Assignment_2_SRP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment - 2 on BPTT"
      ],
      "metadata": {
        "id": "IqizWm2kTSNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "zRq7eYNCTRTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ofypKp-cTZlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset"
      ],
      "metadata": {
        "id": "Y0AZpg9GTbl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "# Load data from JSONL file\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            example = json.loads(line)\n",
        "            data.append(example)\n",
        "    return data\n",
        "\n",
        "# Load training and test data\n",
        "train_data = load_data('/content/train.jsonl')\n",
        "\n",
        "# Print a few rows of the training data\n",
        "print(\"Train data\")\n",
        "for i in range(5):\n",
        "    print(train_data[i])\n",
        "\n",
        "# Load training and test data\n",
        "test_data = load_data('/content/test.jsonl')\n",
        "\n",
        "# Print a few rows of the test data\n",
        "print(\"Test data\")\n",
        "for i in range(5):\n",
        "    print(test_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5CHUVFsqJ3O",
        "outputId": "cd163eda-0f8d-46ae-a046-82b9db0e5afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data\n",
            "{'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'chunk_tags': [1, 1, 1, 0, 1, 1, 1, 0, 1], 'pos_tags': [1, 4, 3, 1, 4, 4, 3, 1, 4]}\n",
            "{'tokens': ['Peter', 'Blackburn'], 'chunk_tags': [1, 0], 'pos_tags': [1, 1]}\n",
            "{'tokens': ['BRUSSELS', '1996-08-22'], 'chunk_tags': [1, 0], 'pos_tags': [1, 4]}\n",
            "{'tokens': ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.'], 'chunk_tags': [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1], 'pos_tags': [2, 1, 1, 4, 4, 1, 4, 4, 4, 3, 1, 4, 1, 4, 4, 3, 1, 4, 1, 4, 4, 3, 1, 1, 4, 4, 4, 4, 1, 4]}\n",
            "{'tokens': ['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'], 'chunk_tags': [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1], 'pos_tags': [1, 4, 1, 4, 2, 1, 1, 4, 3, 1, 1, 1, 4, 4, 1, 1, 4, 4, 1, 4, 1, 3, 4, 1, 4, 2, 3, 1, 4, 3, 4]}\n",
            "Test data\n",
            "{'tokens': ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.'], 'chunk_tags': [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1], 'pos_tags': [1, 4, 1, 4, 1, 1, 4, 1, 4, 2, 1, 4]}\n",
            "{'tokens': ['Nadim', 'Ladki'], 'chunk_tags': [1, 0], 'pos_tags': [1, 1]}\n",
            "{'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06'], 'chunk_tags': [1, 1, 1, 0, 0, 0], 'pos_tags': [1, 4, 1, 1, 1, 4]}\n",
            "{'tokens': ['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.'], 'chunk_tags': [1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1], 'pos_tags': [1, 4, 2, 1, 4, 4, 3, 1, 1, 4, 2, 3, 4, 4, 4, 1, 4, 2, 1, 1, 1, 1, 4, 1, 4]}\n",
            "{'tokens': ['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.'], 'chunk_tags': [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1], 'pos_tags': [4, 1, 4, 4, 1, 4, 4, 4, 2, 1, 1, 4, 2, 1, 4, 4, 4, 2, 1, 4, 1, 4, 1, 1, 4]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Ie6ggFAaVjY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to one-hot encode POS tags with size 5\n",
        "def one_hot_encode_prev(pos_tag):\n",
        "    one_hot = [0, 0, 0, 0, 0]\n",
        "    one_hot[pos_tag] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Function to one-hot encode POS tags with size 4\n",
        "def one_hot_encode_curr(pos_tag):\n",
        "    one_hot = [0, 0, 0, 0]\n",
        "    one_hot[pos_tag - 1] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    x = []\n",
        "    y = []\n",
        "    for sample in data:\n",
        "        tokens = sample['tokens']\n",
        "        pos_tags = sample['pos_tags']\n",
        "        chunk_tags = sample['chunk_tags']\n",
        "\n",
        "        input_seq = []\n",
        "        target_seq = []\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            # One-hot encode previous and current words\n",
        "            prev_word = one_hot_encode_prev(pos_tags[i - 1]) if i > 0 else [1, 0, 0, 0, 0]\n",
        "            current_word = one_hot_encode_curr(pos_tags[i])\n",
        "\n",
        "            # Concatenate previous and current word one-hot encodings\n",
        "            input_seq.append(prev_word + current_word)\n",
        "\n",
        "            # Append chunk tag to target sequence\n",
        "            target_seq.append(chunk_tags[i])\n",
        "\n",
        "        x.append(input_seq)\n",
        "        y.append(target_seq)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "q6LAFTWn1XrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Recurrent Perceptron"
      ],
      "metadata": {
        "id": "ItmQg_9aTh5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK2eS3_UYEKz"
      },
      "outputs": [],
      "source": [
        "class SingleRecurrentPerceptron:\n",
        "    def __init__(self, input_size=9, learning_rate=0.0005):\n",
        "        self.input_size = input_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W_in = np.random.randn(1, input_size)\n",
        "        self.W_rec = np.random.randn(1, 1)\n",
        "        self.b = np.zeros((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.prev_hidden = 0  # Initialize previous hidden state\n",
        "        self.hidden_states = []  # Store hidden states\n",
        "        self.hidden_states.append(0)\n",
        "        self.outputs = []  # Store outputs\n",
        "\n",
        "        # Forward pass through the SRP\n",
        "        for t in range(len(x)):\n",
        "            net_input = np.dot(self.W_in, x[t]) + np.dot(self.W_rec, self.prev_hidden) + self.b\n",
        "            hidden_state = self.sigmoid(net_input)\n",
        "            self.hidden_states.append(hidden_state)\n",
        "            self.outputs.append(hidden_state)\n",
        "            self.prev_hidden = hidden_state\n",
        "        return self.outputs\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        # Backpropagation through time\n",
        "        delta_out = [target - output for output, target in zip(self.outputs, y)]\n",
        "\n",
        "        dW_in = np.zeros_like(self.W_in)\n",
        "        dW_rec = np.zeros_like(self.W_rec)\n",
        "        db = np.zeros_like(self.b)\n",
        "\n",
        "        for t in range(len(x)):\n",
        "          delta_hidden = delta_out[t]\n",
        "          dW_rec += delta_hidden * self.hidden_states[t]\n",
        "          dW_in += delta_hidden * x[t].reshape(1, -1)\n",
        "          db += delta_hidden\n",
        "          for i in range(t-1,-1,-1):\n",
        "            delta_hidden *= self.W_rec * self.outputs[i] * (1-self.outputs[i])\n",
        "            dW_rec += delta_hidden * self.hidden_states[i]\n",
        "            dW_in +=  delta_hidden * x[i].reshape(1, -1)\n",
        "            db += delta_hidden\n",
        "\n",
        "        # Update weights\n",
        "        self.W_in += self.learning_rate * dW_in/len(x)\n",
        "        self.W_rec += self.learning_rate * dW_rec/len(x)\n",
        "        self.b += self.learning_rate * db/len(x)\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Avoid overflow by clipping large values\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(ground_truth, predictions):\n",
        "    accuracy = accuracy_score(ground_truth, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predictions, average='weighted',zero_division=1)\n",
        "    class_wise_precision, class_wise_recall, class_wise_f1, _ = precision_recall_fscore_support(ground_truth, predictions, average=None, zero_division=1)\n",
        "\n",
        "    # Compute class-wise accuracy\n",
        "    class_wise_accuracy = []\n",
        "    for i in range(len(class_wise_precision)):\n",
        "        class_wise_accuracy.append(class_wise_precision[i] * class_wise_recall[i] / class_wise_f1[i] if class_wise_f1[i] != 0 else 0)\n",
        "\n",
        "    return accuracy, precision, recall, f1, class_wise_precision, class_wise_recall, class_wise_f1, class_wise_accuracy\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    return [item for sublist in list_of_lists for item in sublist]"
      ],
      "metadata": {
        "id": "H223puk43UyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess training data\n",
        "train_input, train_target = preprocess_data(train_data)\n",
        "\n",
        "# Preprocess test data\n",
        "test_input, test_target = preprocess_data(test_data)"
      ],
      "metadata": {
        "id": "Apkq1Nvt3txO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training 5-fold cross validation"
      ],
      "metadata": {
        "id": "986zg9j0TrJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Initialize lists to store performance metrics across all folds\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "# Define number of folds for cross-validation\n",
        "num_folds = 5\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
        "\n",
        "# Initialize variables to store the best model and its accuracy\n",
        "best_accuracy = -1\n",
        "best_model = None\n",
        "\n",
        "# Perform cross-validation\n",
        "for fold_number, (train_index, val_index) in enumerate(kf.split(train_input), 1):\n",
        "    print(f\"Fold {fold_number}:\")\n",
        "    # Initialize the SingleRecurrentPerceptron for this fold\n",
        "    model = SingleRecurrentPerceptron()\n",
        "\n",
        "    # Extract training and validation data for this fold\n",
        "    X_train_fold = [train_input[i] for i in train_index]\n",
        "    X_val_fold = [train_input[i] for i in val_index]\n",
        "    y_train_fold = [train_target[i] for i in train_index]\n",
        "    y_val_fold = [train_target[i] for i in val_index]\n",
        "\n",
        "    # Train the model on the current training fold\n",
        "    num_epochs = 10\n",
        "    prev_loss = 10000     # Arbitrary large\n",
        "    for epoch in range(num_epochs):\n",
        "      total_loss = 0\n",
        "      for i in range(len(X_train_fold)):\n",
        "        x = np.array(X_train_fold[i])\n",
        "        y_true = np.array(y_train_fold[i])\n",
        "\n",
        "        y_pred = np.array(model.forward(x))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        total_loss += loss\n",
        "\n",
        "        model.backward(x, y_true)\n",
        "\n",
        "      avg_loss = total_loss / len(X_train_fold)\n",
        "      if (avg_loss > prev_loss):\n",
        "        break;\n",
        "      prev_loss = avg_loss\n",
        "      print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
        "\n",
        "    # Validate the model on the current validation fold\n",
        "    ground_truth = flatten(y_val_fold)\n",
        "    predictions = []\n",
        "    threshold = 0.5\n",
        "    for i in range(len(X_val_fold )):\n",
        "        x = np.array(X_val_fold [i])\n",
        "        y_pred = model.forward(x)\n",
        "        y_pred = [1 if value >= threshold else 0 for value in y_pred]\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "    predictions = flatten(predictions)\n",
        "\n",
        "    accuracy, precision, recall, f1, class_wise_precision, class_wise_recall, class_wise_f1, class_wise_accuracy = calculate_metrics(ground_truth, predictions)\n",
        "\n",
        "    # Check if this fold has the best accuracy so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        # Store the best model\n",
        "        best_model = model\n",
        "\n",
        "    # Store performance metrics for this fold\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Calculate mean performance metrics across all folds\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "mean_precision = np.mean(precisions)\n",
        "mean_recall = np.mean(recalls)\n",
        "mean_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Accuracy:\", mean_accuracy)\n",
        "print(\"Mean Precision:\", mean_precision)\n",
        "print(\"Mean Recall:\", mean_recall)\n",
        "print(\"Mean F1 Score:\", mean_f1_score)\n",
        "\n",
        "print(\"Accuracies: \", accuracies)"
      ],
      "metadata": {
        "id": "LrIzQz40zaom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5d944e-d501-4e0c-a74a-731c7f291c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1:\n",
            "Epoch 1, Average Loss: 0.8995608251799676\n",
            "Epoch 2, Average Loss: 0.8448879515362022\n",
            "Epoch 3, Average Loss: 0.8333566233142405\n",
            "Epoch 4, Average Loss: 0.8309525240551755\n",
            "Fold 2:\n",
            "Epoch 1, Average Loss: 0.878939359458931\n",
            "Epoch 2, Average Loss: 0.8143528293044621\n",
            "Epoch 3, Average Loss: 0.7976440532361837\n",
            "Epoch 4, Average Loss: 0.7933155284051989\n",
            "Fold 3:\n",
            "Epoch 1, Average Loss: 0.7301313131952497\n",
            "Epoch 2, Average Loss: 0.694963467785395\n",
            "Epoch 3, Average Loss: 0.6834476006187388\n",
            "Fold 4:\n",
            "Epoch 1, Average Loss: 0.7438824153888468\n",
            "Epoch 2, Average Loss: 0.7232081934816477\n",
            "Epoch 3, Average Loss: 0.7185729777843893\n",
            "Fold 5:\n",
            "Epoch 1, Average Loss: 0.7268785642741791\n",
            "Epoch 2, Average Loss: 0.7135362978143811\n",
            "Mean Accuracy: 0.8253095372343171\n",
            "Mean Precision: 0.8300079772121418\n",
            "Mean Recall: 0.8253095372343171\n",
            "Mean F1 Score: 0.8173119285176884\n",
            "Accuracies:  [0.8120929766556457, 0.8672761508349438, 0.8672835078659207, 0.8084552685848964, 0.7714397822301787]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print final weights\n",
        "print(\"Final Weights:\")\n",
        "print(\"W_in:\", best_model.W_in)\n",
        "print(\"W_rec:\", best_model.W_rec)\n",
        "print(\"b:\", best_model.b)"
      ],
      "metadata": {
        "id": "0KCUYqJTV0xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2218b5c1-3807-49b4-b2bc-32b5924b40be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights:\n",
            "W_in: [[ 0.86852614  0.21277101  0.47042728  0.20162258  1.00052214 -0.91715702\n",
            "   0.10022747 -0.90133645  0.11281401]]\n",
            "W_rec: [[-0.28455012]]\n",
            "b: [[0.40482361]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "pZDH2Vct7iKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = flatten(test_target)\n",
        "predictions = []\n",
        "threshold = 0.5\n",
        "for i in range(len(test_input)):\n",
        "        x = test_input[i]\n",
        "        y_pred = best_model.forward(x)\n",
        "        y_pred = [1 if value > threshold else 0 for value in y_pred]\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "predictions = flatten(predictions)\n",
        "\n",
        "# Calculate metrics for test data\n",
        "test_accuracy, test_precision, test_recall, test_f1, test_class_precision, test_class_recall, test_class_f1,class_wise_accuracy = calculate_metrics(ground_truth, predictions)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "print(\"Test F1 Score:\", test_f1)\n",
        "print(\"Test Class-wise Accuracy:\", class_wise_accuracy)\n",
        "print(\"Test Class-wise Precision:\", test_class_precision)\n",
        "print(\"Test Class-wise Recall:\", test_class_recall)\n",
        "print(\"Test Class-wise F1 Score:\", test_class_f1)"
      ],
      "metadata": {
        "id": "iT_whWLkbkg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fabaa7-c1ad-4cf1-8bc4-6ba13c980238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8432217077635404\n",
            "Test Precision: 0.8677589661258255\n",
            "Test Recall: 0.8432217077635404\n",
            "Test F1 Score: 0.8301726257777929\n",
            "Test Class-wise Accuracy: [0.7699877657753113, 0.9012030441231789]\n",
            "Test Class-wise Precision: [0.9763358  0.80970994]\n",
            "Test Class-wise Recall: [0.56363974 0.99269615]\n",
            "Test Class-wise F1 Score: [0.71468882 0.89191436]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language constraints inequalities"
      ],
      "metadata": {
        "id": "blnWt82XznI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "inequalities = [\n",
        "    (\"V_cap + WDT > b\", V_cap + WDT > b, V_cap + WDT - b),\n",
        "    (\"V_cap + WJJ > b\", V_cap + WJJ > b, V_cap + WJJ - b),\n",
        "    (\"V_cap + WNN > b\", V_cap + WNN > b, V_cap + WNN - b),\n",
        "    (\"V_cap + WOT > b\", V_cap + WOT > b, V_cap + WOT - b),\n",
        "    (\"W_rec + VDT + WJJ < b\", W_rec + VDT + WJJ < b, (W_rec + VDT + WJJ) - b),\n",
        "    (\"W_rec + VDT + WNN < b\", W_rec + VDT + WNN < b, (W_rec + VDT + WNN) - b),\n",
        "    (\"VJJ + WJJ < b\", VJJ + WJJ < b, VJJ + WJJ - b),\n",
        "    (\"VJJ + WNN < b\", VJJ + WNN < b, VJJ + WNN - b),\n",
        "    (\"W_rec + VJJ + WJJ < b\", W_rec + VJJ + WJJ < b, (W_rec + VJJ + WJJ) - b),\n",
        "    (\"W_rec + VJJ + WNN < b\", W_rec + VJJ + WNN < b, (W_rec + VJJ + WNN) - b),\n",
        "    (\"VNN + WOT > b\", VNN + WOT > b, VNN + WOT - b),\n",
        "    (\"W_rec + VNN + WOT > b\", W_rec + VNN + WOT > b, (W_rec + VNN + WOT) - b),\n",
        "    (\"W_rec + VOT + WDT > b\", W_rec + VOT + WDT > b, (W_rec + VOT + WDT) - b),\n",
        "    (\"W_rec + VOT + WJJ > b\", W_rec + VOT + WJJ > b, (W_rec + VOT + WJJ) - b),\n",
        "    (\"W_rec + VOT + WNN > b\", W_rec + VOT + WNN > b, (W_rec + VOT + WNN) - b),\n",
        "    (\"W_rec + VOT + WOT > b\", W_rec + VOT + WOT > b, (W_rec + VOT + WOT) - b)\n",
        "]\n",
        "\n",
        "# Extracting expressions, boolean values, and results\n",
        "expressions = [inequality[0] for inequality in inequalities]\n",
        "boolean_values = [inequality[1] for inequality in inequalities]\n",
        "results = [inequality[2] for inequality in inequalities]\n",
        "\n",
        "# Creating a table\n",
        "table = zip(expressions, boolean_values, results)\n",
        "print(tabulate(table, headers=[\"Expression\", \"Boolean Value\", \"Difference\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_7shempdISe",
        "outputId": "b306fdc2-4d82-4d64-97f7-5d8cd8f02f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression               Boolean Value    Difference\n",
            "---------------------  ---------------  ------------\n",
            "V_cap + WDT > b                      1      1.37358\n",
            "V_cap + WJJ > b                      1      0.372013\n",
            "V_cap + WNN > b                      1      0.356193\n",
            "V_cap + WOT > b                      1      1.38616\n",
            "W_rec + VDT + WJJ < b                1     -0.310636\n",
            "W_rec + VDT + WNN < b                1     -0.326456\n",
            "VJJ + WJJ < b                        1     -0.29489\n",
            "VJJ + WNN < b                        1     -0.310711\n",
            "W_rec + VJJ + WJJ < b                1     -0.57944\n",
            "W_rec + VJJ + WNN < b                1     -0.595261\n",
            "VNN + WOT > b                        1      0.730409\n",
            "W_rec + VNN + WOT > b                1      0.445859\n",
            "W_rec + VOT + WDT > b                1      1.22102\n",
            "W_rec + VOT + WJJ > b                1      0.219459\n",
            "W_rec + VOT + WNN > b                1      0.203639\n",
            "W_rec + VOT + WOT > b                1      1.23361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo"
      ],
      "metadata": {
        "id": "8J8zPR0CFNPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "metadata": {
        "id": "pQz0MccjFk0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Perform POS tagging\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define mapping of POS tags to categories\n",
        "pos_to_category = {\n",
        "    'NN': 1,  # Nouns\n",
        "    'JJ': 3,  # Adjectives\n",
        "    'DT': 2,  # Determiners\n",
        "}\n",
        "\n",
        "# Function to map POS tags to categories\n",
        "def map_tag_to_category(tag):\n",
        "    if tag.startswith('NN'): return pos_to_category['NN']\n",
        "    elif tag.startswith('JJ'): return pos_to_category['JJ']\n",
        "    elif tag.startswith('DT'): return pos_to_category['DT']\n",
        "    else: return 4  # Other\n",
        "\n",
        "demo_pos_tags = []\n",
        "# Perform mapping and print results\n",
        "for token, tag in tagged_tokens:\n",
        "    category = map_tag_to_category(tag)\n",
        "    demo_pos_tags.append(category)\n",
        "\n",
        "demo_pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_CUfqeVFKUZ",
        "outputId": "0701ed15-b9bb-4766-e63f-d40a228563bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 1, 1, 4, 4, 2, 3, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EkNYof-gFTFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}